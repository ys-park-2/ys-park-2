### 의사결정나무
   - 데이터에 내재되어 있는 패턴을 변수의 조합으로 나타내는 예측/분류 모델을 나무의 형태로 만드는 것
   - 질문을 던져서 맞고 틀리는 것에 따라 우리가 생각하고 있는 대상을 좁혀나감 (이진분할 반복)

  - Data 
    -> Algorithm : 데이터를 2개 혹은 그 이상의 부분집합으로 분할(데이터가 균일해지도록 분할)
                   분류 (비슷한 범주를 갖고 있는 관측치끼리 모음)
                   예측 (비슷한 수치를 갖고 있는 관측치끼리 모음)
    -> Model(Output) :
    ![image](https://user-images.githubusercontent.com/79842387/111979441-6bf42280-8b48-11eb-9e38-a953cd2062ac.png)

1. Regression Tree (예측나무모델)
  - y는 숫자
  - indicator function (집합에 포함되는지, 0: false, 1 : true)

  - 예측 나무 모델링 프로세스 : 각 분할에 속해 있는 y값들의 평균으로 예측했을 때 오류가 최소, grid search
  - ![image](https://user-images.githubusercontent.com/79842387/111981145-a19a0b00-8b4a-11eb-9e7b-20d5287068e6.png)


2. Classification Tree (분류나무모델)
  - y는 범주형
  - 각 관측치마다 반응변수 값 즉, k개의 클래스(범주) 존재
  - k(m) : m번째 관측치에서 가장 큰 확률을 갖는 범주
  -  ![image](https://user-images.githubusercontent.com/79842387/111983456-7f55bc80-8b4d-11eb-96b2-7e39b270521e.png)
     ![image](https://user-images.githubusercontent.com/79842387/111984096-49650800-8b4e-11eb-8f9c-f90a8ebcb047.png)

  - 분류모델에서의 비용함수 (불순도 측정)
    1) Misclassification rate : 매칭되지 않는 것이 최소로
    ![image](https://user-images.githubusercontent.com/79842387/111984885-49193c80-8b4f-11eb-96ec-41680d405c12.png)

    2) Gini Index : 0~ 0.5 불순도 괜찮음
    ![image](https://user-images.githubusercontent.com/79842387/111985061-8251ac80-8b4f-11eb-8cd6-3360d95af994.png)

    4) Cross-entropy (Information Gain) : entropy는 1에 가까울 수록 안좋음
        - entropy 더 감소시키는 변수가 중요도 높은 변수 (Information Gain 높을수록 중요한 변수)
    

  - 분류나무 모델링 프로세스
    - 분할변수(j)와 분할점(s)는 어떻게 결정할까?
    
    
 - 분할 법칙
  1. 분할 변수와 분할기준(점)은 목표변수의 분포를 가장 잘 구별해주는 쪽으로 정합
  2. 목표변수의 부노를 잘 구별해주는 측도로 순수도(purity) 또는 불순도(impurity)를 정의
  3. 예를 들어 클래스 0와 클래스 1의 비율이 45%와 55%인 노트는 
     각클래스의 비율이 90%와 10%인 마디에 비해 순수도가 낮다(불순도가 높다)라고 해석
  4. 각 노드에서 분할변수와 분할점의 설정은 불순도이 감소가 최대가 되도록 선택

### 개별 트리 모델의 단점

  - 계층적 구조로 인해 중간에 에러가 발생하면 다음 단계로 에러 계속 전파
  - 학습 데이터 미세한 변동에도 최종 결과 크게 영향
  - 적은 개수의 노이즈에도 크게 영향
  - 나무의 최종노드 개수를 늘리면 과적합 위험 (Low Bias, Large Variane)
  -> 해결방안 : Random Forest
