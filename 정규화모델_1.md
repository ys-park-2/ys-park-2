 ### 좋은 모델이란?
  1) 현재 데이터 (training data)를 잘 설명하는 모델 : Explanatory modeling
    = Training error 를 minimize하는 모델
    
  2) 미래 데이터 (testing data)에 대한 예측 성능이 좋은 모델 : Predictibe modeling
    = 미래 데이터에 대한 expected error가 낮은 모델
    - expected MSE 를 줄이려면 bias, variance 혹은 둘다 낮춰야함, 그렇지 못하다면 하나라도 작으면 좋음
    - Bias가 증가되더라도 variance 감소폭이 더 크다면 expected MSE는 감소 (예측 성능 증가)
   
    best : 1 / worst : 4
    ![image](https://user-images.githubusercontent.com/79842387/111631710-11508300-8837-11eb-8529-7ab828194dae.png)

  
###최소 제곱법 (Least squares estimation method) - unbiased
![image](https://user-images.githubusercontent.com/79842387/111632051-70ae9300-8837-11eb-913e-82e378c587ac.png)

- subset selection
-> 전체 변수 중 일부만을 선택함에 따라 bias 증가할 수 있지만, variance 감소 
![image](https://user-images.githubusercontent.com/79842387/111632361-bcf9d300-8837-11eb-96e0-bc7e59103911.png)


###정규화
  - ![image](https://user-images.githubusercontent.com/79842387/111632849-3d203880-8838-11eb-8944-40f0652d316d.png)
  - 3번 -> 2번으로 가는 방법은?

  ![image](https://user-images.githubusercontent.com/79842387/111633298-ae5feb80-8838-11eb-866f-0dc859ca2a19.png)

  - 람다를 굉장히 크게 하면? -> 베타 값이 0 이 됨, Underfitting!
  - 람다를 작게 하면? -> 최소제곱법이랑 동일해짐, 제약이 거의 없어짐, Overfitting!

  - Regulariztion Method는 회귀계수 beta가 가질 수 있는 값에 제약조건을 부여하는 방법
    (최소제곱법과의 차이는 베타에 대한 제약이 있느냐 없느냐)
  - ![image](https://user-images.githubusercontent.com/79842387/111633619-0bf43800-8839-11eb-93eb-9e8443419806.png)

  - Ridge Regression
    - t, 람다의 역할은 같다, 하이퍼파라미터임 (제약을 거는 것, 위는 제약식이고 아래는 아님)
![image](https://user-images.githubusercontent.com/79842387/111633861-4958c580-8839-11eb-84cf-d4aa74ec91ea.png)

  - MSE 는 타원 모양
  - t값이 크면 제약이 거의 없는 것 (최소제곱법의 베타값과 동일할 수 있음)
  - t값이 작아질 수록 베타값도 작아짐
  - Ridge는 행렬연산을 통해 closed form solution을 구할 수 있음
  ![image](https://user-images.githubusercontent.com/79842387/111634777-2c70c200-883a-11eb-915d-2bcd78232230.png)

